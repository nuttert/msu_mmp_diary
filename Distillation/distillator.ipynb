{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.compat.v1 as tf \n",
    "import tensorflow_addons as tfa\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D,Input\n",
    "from  tensorflow.keras import backend as K\n",
    "from numpy import random\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "plt.rcParams['axes.facecolor'] = '#232530'\n",
    "plt.rcParams['figure.facecolor'] = '#232530'\n",
    "tf.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "X, y = fetch_openml('mnist_784', version=1, return_X_y=True)\n",
    "X = np.array(X)/255\n",
    "y = np.array(y)\n",
    "y = np.eye(10)[np.array(y,dtype=np.int16)]\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "num_classes = 10\n",
    "epochs = 10\n",
    "\n",
    "img_rows, img_cols = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LeNet:\n",
    "    def __init__(self,num_classes,sigma=0.2,mu=0):\n",
    "        self.num_classes = num_classes\n",
    "        self.sigma = sigma\n",
    "        self.mu = mu\n",
    "\n",
    "        self.size = 28\n",
    "        self.layers_shapes = [(self.size*self.size,num_classes)]\n",
    "        self.layers_sizes_w = [np.prod(shape) for shape in self.layers_shapes]\n",
    "        self.layers_sizes_b = [num_classes]\n",
    "        \n",
    "        self.initializers = None\n",
    "        self.weights = None\n",
    "        self.weights_size = sum(self.layers_sizes_w)\n",
    "        self.biases_size = sum(self.layers_sizes_b)\n",
    "        self.common_size = self.weights_size+self.biases_size\n",
    "    \n",
    "    def forward(self,x, weights=None):\n",
    "        if(weights != None):\n",
    "            weights = tf.squeeze(weights)\n",
    "            assert weights.shape[0] == self.common_size\n",
    "            self.weights = weights\n",
    "            weights,biases = self.unflat_weights(weights)\n",
    "        elif self.weights == None:\n",
    "            weights = tf.truncated_normal(shape=[self.weights_size],\n",
    "                                          mean = self.mu, stddev = self.sigma)\n",
    "            biases = tf.zeros(shape=[self.biases_size])\n",
    "    \n",
    "            self.weights = tf.Variable(tf.concat([weights,biases],axis=0))\n",
    "            weights, biases = self.unflat_weights(self.weights)\n",
    "            self.initializers = [self.weights.initializer]\n",
    "\n",
    "        \n",
    "        flatten = tf.reshape(x,[-1,self.size*self.size])\n",
    "\n",
    "        out = tf.add(tf.matmul(flatten, weights[0]), biases[0])\n",
    "        self.out = out\n",
    "        return out\n",
    "\n",
    "    \n",
    "    def flat_weights(self,weights,biases):\n",
    "        weights = weights+biases\n",
    "        flatten = None\n",
    "        for weight in weights:\n",
    "            weight = tf.reshape(weight,[-1])\n",
    "            flatten = tf.concat([flatten,weight],axis=0) if flatten != None else weight\n",
    "        return flatten\n",
    "            \n",
    "    def unflat_weights(self,weights):\n",
    "            layers_shapes = self.layers_shapes\n",
    "            layers_sizes_w = self.layers_sizes_w\n",
    "            layers_sizes_b = self.layers_sizes_b\n",
    "            \n",
    "            \n",
    "            biases = weights[self.weights_size:]\n",
    "            weights=weights[:self.weights_size]\n",
    "            unflatten_weights = []\n",
    "            unflatten_biases = []\n",
    "            \n",
    "            for i in np.arange(len(layers_sizes_w)):\n",
    "                from_ = sum(layers_sizes_w[:i])\n",
    "                to_ = sum(layers_sizes_w[:i+1])\n",
    "                unflatten_weights.append(tf.reshape(weights[from_:to_],layers_shapes[i]))\n",
    "                \n",
    "            for i in np.arange(len(layers_sizes_b)):\n",
    "                from_ = sum(layers_sizes_b[:i])\n",
    "                to_ = sum(layers_sizes_b[:i+1])\n",
    "                unflatten_biases.append(biases[from_:to_])\n",
    "                \n",
    "            return unflatten_weights,unflatten_biases\n",
    "            \n",
    "        \n",
    "    def get_weights(self):\n",
    "        return self.weights\n",
    "        \n",
    "    def __call__(X):\n",
    "        return self.out(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Distillator:\n",
    "    def __init__(self,models,\n",
    "                 shape,num_classes,M,\n",
    "                 batch_size,num_epochs,distillEpoches,\n",
    "                 sigma=0.1,mu=0):\n",
    "        self.batch_size = batch_size\n",
    "        self.num_epochs = num_epochs\n",
    "        self.models = models\n",
    "        self.num_classes = num_classes\n",
    "        self.distillEpoches = distillEpoches\n",
    "        \n",
    "        self.learningRateX = tf.constant(1e-2,dtype=tf.float32)\n",
    "        self.learningRateW = tf.Variable(1e-1,dtype=tf.float32)\n",
    "\n",
    "        self.x_real = tf.placeholder(tf.float32, shape=[None, *shape])\n",
    "        self.y_real = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
    "\n",
    "        self.x_distilled = tf.Variable(tf.truncated_normal([M, *shape] ,stddev=sigma,mean=mu))\n",
    "        self.y_distilled = np.eye(num_classes)[np.arange(0,num_classes)]\n",
    "        self.y_distilled = tf.Variable(self.y_distilled)\n",
    "        self.statistics = {\"loss\":[]}\n",
    "    \n",
    "    def forward(self,model):\n",
    "            model.forward(self.x_distilled)\n",
    "            weights = model.get_weights()\n",
    "            (new_weights_list,gradients_list) = [weights],[]\n",
    "\n",
    "            for epoch in np.arange(0,self.distillEpoches):\n",
    "                output = model.forward(self.x_distilled,weights=weights)\n",
    "                loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output,\n",
    "                                                                                 labels=self.y_distilled))\n",
    "                dloss_dw = tf.gradients(loss,weights)\n",
    "                dloss_dw = tf.squeeze(dloss_dw)\n",
    "                new_weights = weights - self.learningRateW*dloss_dw\n",
    "                    \n",
    "                new_weights_list.append(new_weights)\n",
    "                gradients_list.append(self.learningRateW*dloss_dw)\n",
    "                weights = new_weights\n",
    "                \n",
    "            self.__weights_initializers += model.initializers\n",
    "            output = model.forward(self.x_real,weights=weights)\n",
    "            loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=output,\n",
    "                                                                             labels=self.y_real))\n",
    "            return(loss,\n",
    "                   new_weights_list,\n",
    "                   gradients_list)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def backward(self,forwardOutput):\n",
    "        (loss,new_weights_list,\n",
    "                gradients_list)  =  forwardOutput\n",
    "        \n",
    "        dloss_dw_last, = tf.gradients(loss,new_weights_list[-1])\n",
    "        x_distilled_grad = tf.zeros_like(self.x_distilled)\n",
    "        lr_grad = 0\n",
    "        \n",
    "        for weights,dloss_dw in reversed(list(zip(new_weights_list,gradients_list))):\n",
    "            dloss_dw_last = dloss_dw_last*-1\n",
    "            gradients = tf.gradients(dloss_dw,\n",
    "                                   [self.x_distilled,\n",
    "                                    self.learningRateW,\n",
    "                                    weights],\n",
    "                                    grad_ys=dloss_dw_last)\n",
    "            (dloss_dx,dloss_dlr,ddloss_ddw) = gradients\n",
    "            \n",
    "            x_distilled_grad += dloss_dx\n",
    "            lr_grad += dloss_dlr\n",
    "            \n",
    "            dloss_dw_last = dloss_dw_last+ddloss_ddw\n",
    "        return x_distilled_grad,lr_grad\n",
    "            \n",
    "            \n",
    "    def optimizer(self,gradients):\n",
    "        x_grad_sum = tf.zeros_like(self.x_distilled)\n",
    "        lr_grad_sum = 0\n",
    "        for x_grad,lr_grad in gradients:\n",
    "            x_grad_sum = x_grad_sum+x_grad\n",
    "            lr_grad_sum= lr_grad_sum+lr_grad\n",
    "        x_grad = x_grad_sum/len(self.models)\n",
    "        lr_grad = lr_grad_sum/len(self.models)\n",
    "        \n",
    "        self.x_grad = x_grad\n",
    "        self.lr_grad = lr_grad\n",
    "        self.optimize_x = tf.assign_sub(self.x_distilled,self.learningRateX*x_grad)\n",
    "        self.optimize_lr = tf.assign_sub(self.learningRateW,self.learningRateX*lr_grad)\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "    def train(self,X,y):\n",
    "        sess = tf.Session()\n",
    "        self.sess = sess\n",
    "        sess.run([self.x_distilled.initializer,\n",
    "                  self.y_distilled.initializer,\n",
    "                  self.learningRateW.initializer])\n",
    "        \n",
    "        self.__weights_initializers = []\n",
    "        \n",
    "        gradients = []\n",
    "        lossSum = 0\n",
    "        \n",
    "        for model in self.models:\n",
    "            forwardOutput = self.forward(model)\n",
    "            loss,_,_  = forwardOutput\n",
    "            lossSum = lossSum+loss\n",
    "            gradients.append(self.backward(forwardOutput))\n",
    "        self.optimizer(gradients)\n",
    "        \n",
    "        for epoch in np.arange(0,self.num_epochs):\n",
    "            for step_i in np.arange(0,X.shape[0], self.batch_size):\n",
    "                x_real_batch = X[step_i:step_i + self.batch_size,:]\n",
    "                y_real_batch = y[step_i:step_i + self.batch_size,:]\n",
    "                \n",
    "                sess.run(self.__weights_initializers)\n",
    "                \n",
    "                sess.run([self.optimize_x,self.optimize_lr],\n",
    "                        feed_dict={self.x_real:x_real_batch,\n",
    "                                                    self.y_real:y_real_batch})\n",
    "\n",
    "            self.statistics[\"loss\"].append(\n",
    "            sess.run(lossSum,\n",
    "                     feed_dict={self.x_real:x_real_batch,\n",
    "                                                    self.y_real:y_real_batch}))\n",
    "                                \n",
    "                \n",
    "                \n",
    "                \n",
    "    def getDistilledData(self):\n",
    "        return self.sess.run([self.x_distilled,self.y_distilled])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "distillator = Distillator([LeNet(10)],[28,28,1],10,10,\n",
    "                          batch_size,num_epochs=50,distillEpoches=3)\n",
    "distillator.train(X_train.reshape(X_train.shape[0],28,28,1),\n",
    "                  y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x__,y__ = distillator.getDistilledData()\n",
    "\n",
    "gridsize = (2, 5)\n",
    "fig = plt.figure(figsize=(15, 8))\n",
    "for i in range(10):\n",
    "    ax = plt.subplot2grid(gridsize, (i//5, i%5))\n",
    "    ax.imshow(x__.reshape(10,28,28)[i])\n",
    "    plt.xlabel(\"Label:{0}\".format(i))\n",
    "plt.savefig('figs.jpg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
