Не успел все результаты затехать пока что. Но у меня несколько проблем, всё та же проблема
с мягкими метками - из-за кроссэнтропии у меня не получается делать оптимизацию по меткам, поскольку градиенты 
по меткам всегда равны нулю. И вторая проблема - на однослойной сети получается дистилляция качественнее, чем
на многослойной сети по типу LeNet, как у авторов исходной статьи. По результатам визуализаци, в принципе, совпадает с ними.

Написал реализацию на tensorflow...как минимум, чтобы не копипастить у авторов, а самому всё с нуля реализовать.
Не знаю, нормально ли то, что в отчете содержится код, но, мне показалось, во многих статьях не хватает кода и его разбора,
и код у авторов намного шире, чем описанное в статье. Наверное, здесь идет привязка к языку и фреймворку...поэтому многие
дают алгоритмы и куски псведокода, но это слишком непрактично.

(https://arxiv.org/pdf/1708.08689.pdf - Отравление + back-gradient optimization,
Инициализация Ксавье,
https://www.groundai.com/project/improving-dataset-distillation/1 - мягкие метки
)

Насчет того вопроса про то, что loss = nan при большом разбросе весов:
По результатам получилось так...делаю один градиентный шаг по весам(а градиент получается достаточно большой у w_0), 
значения новых весов w_1 имеют теперь порядок градиента , в итоге градиент по X из-за w_1 получается уже чрезмерно большим.

Короче говоря, в дистиллирующей модели нельзя брать большой разброс у весов, судя по всему, 
по крайней мере с одной дистиллирующей эпохой)
Выходит как цепная реакция, если веса W большие, то X становится большой сразу же, а когда и то и другое на следующей итерации 
имеет большое значение, то всё ломается)

Я убрал из LeNet пару слоев и она стала менее требовательна к этому разбросу => можем уменьшить время обучения,
когда разброс побольше.
