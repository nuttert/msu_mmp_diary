Не успел все результаты затехать пока что. Но у меня несколько проблем, всё та же проблема
с мягкими метками - из-за кроссэнтропии у меня не получается делать оптимизацию по меткам, поскольку градиенты 
по меткам всегда равны нулю. И вторая проблема - на однослойной сети получается дистилляциякачественнее, чем
на многослойной сети по типу LeNet, как у авторов исходной статьи. По результатам визуализаци, в принципе, совпадает с ними.

Написал реализацию на tensorflow...как минимум, чтобы не копипастить у авторов, а самому всё с нуля реализовать.
Не знаю, нормально ли то, что в отчете содержится код, но, мне показалось, во многих статьях не хватает кода и его разбора,
и код у авторов намного шире, чем описанное в статье. Наверное, здесь идет привязка к языку и фреймворку...поэтому многие
дают алгоритмы и куски псведокода, но это слишком непрактично.

(https://arxiv.org/pdf/1708.08689.pdf - Отравление + back-gradient optimization,
Инициализация Ксавье,
https://www.groundai.com/project/improving-dataset-distillation/1 - мягкие метки
)
