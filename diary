Не успел все результаты затехать пока что. Но у меня несколько проблем, всё та же проблема
с мягкими метками - из-за кроссэнтропии у меня не получается делать оптимизацию по меткам, поскольку градиенты 
по меткам всегда равны нулю. И вторая проблема - на однослойной сети получается дистилляция качественнее, чем
на многослойной сети по типу LeNet, как у авторов исходной статьи. По результатам визуализаци, в принципе, совпадает с ними.

Написал реализацию на tensorflow...как минимум, чтобы не копипастить у авторов, а самому всё с нуля реализовать.
Не знаю, нормально ли то, что в отчете содержится код, но, мне показалось, во многих статьях не хватает кода и его разбора,
и код у авторов намного шире, чем описанное в статье. Наверное, здесь идет привязка к языку и фреймворку...поэтому многие
дают алгоритмы и куски псведокода, но это слишком непрактично.

(https://arxiv.org/pdf/1708.08689.pdf - Отравление + back-gradient optimization,
Инициализация Ксавье,
https://www.groundai.com/project/improving-dataset-distillation/1 - мягкие метки
)

С одной проблемой выше разобрался...это, наверное, даже очевидно, что однослойная сеть на случайной инициализации весов
даст качество лучше, чем LeNet, где большой количество параметров и все параметры инициализируются на каждой итерации
также случайно. В итоге на однослойной сети качество 91.5% на тесте, а на LeNet у авторов 79 +- 6%.
Но при фиксированной инициализации LeNet, вероятно, будет лучше. Еще не тестил это.
